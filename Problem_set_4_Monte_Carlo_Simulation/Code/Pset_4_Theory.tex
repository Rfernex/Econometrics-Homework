\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}

\newcommand\mymapsto{\mathrel{\ooalign{$\rightarrow$\cr%
  \kern-.15ex\raise.275ex\hbox{\scalebox{1}[0.522]{$\mid$}}\cr}}}


\title{Pset 4 - Theory}
\author{Romain Fernex}
\date{March 2025}

\begin{document}

\maketitle

\section{Problem 1}


Let's show that $\hat{\alpha}=\bar{y}-\hat{\beta}\bar{x}\xrightarrow{P}\alpha$ : We know that $\hat{\beta}\xrightarrow{P}\beta$ (consistent estimator)
Using the Law of Large Number, we obtain : 
\begin{equation}
    \bar{y} = \frac{1}{N}\sum_{i=1}^N y_i \xrightarrow{P}E(y_i) = E(\alpha+\beta x_i +u_i) 
\end{equation}
Similarly, based on the Law of Large Number, we also get : 
\begin{equation}
    \bar{x} = \frac{1}{N}\sum_{i=1}^nx_i \xrightarrow[]{P}E(x_i)
\end{equation}
Considering result (2) and the consistency of $\hat{\beta}$, we can use the Slutsky theorem to assert that : 
\begin{equation}
    \hat{\beta}\bar{x} \xrightarrow{P} \beta E(x_i)
\end{equation}
Lastly, using Slutsky with results (1) and (3) this time, we get : 
\begin{equation}
    \hat{\alpha} \xrightarrow{P} E(\alpha + \beta x_i + u_i) - \beta E(x_i) = \alpha + E(u_i) 
\end{equation}
As $E(u_i|x_i)=0$ is a condition for $\hat{\beta}$ to be consistent and since $E(u_i|x_i)=0 \Longrightarrow{} E(u_i)=0$, we have : 
\begin{equation}
    \hat{\alpha} \xrightarrow[]{P} \alpha
\end{equation}
So $\hat{\alpha}$ is a consistent estimator of $\alpha$ !

\section{Problem 2}

We start by proving that : 
\begin{equation}
    \hat{u}_i=(u_i-\bar{u})-\sum_{j=1}^k(\hat{\beta}_j -\beta_j)(x_{ij}-\hat{x}_j)
\end{equation}
We know that : 
\begin{equation}
\begin{aligned}
    \hat{u}_i&=y_i-\hat{y}_i\\
    &= (\beta_0+\sum_{j=1}^k\beta_jx_{ij}+u_i) - (\hat{\beta}_0+\sum_{j=1}^k\hat{\beta}_jx_{ij})\\
    &=(\beta_0 - \hat{\beta}_0)+\sum_{j=1}^k(\beta_j-\hat{\beta}_j)x_{ij} + u_i
\end{aligned}
\end{equation}
Now that we have (7), we can take the sum over all i and divide by n:
\begin{equation}
\begin{aligned}
    \frac{1}{n}\sum_{j=1}^k\hat{u}_i &= (\beta_0-\hat{\beta}_0) + \sum_{j=1}^k(\beta_j-\hat{\beta}_j) [\frac{1}{n}\sum_{i=1}^nx_{ij}] + \bar{u} \\
    \underbrace{\bar{\hat{u}}}_{=0} &= (\beta_0-\hat{\beta}_0) + \sum_{j=1}^k(\beta_j-\hat{\beta}_j)\bar{x_j}+ \bar{u} \\
        \hat{u}_i-0&= \hat{u}_i - (\beta_0-\hat{\beta}_0) - \sum_{j=1}^k(\beta_j-\hat{\beta}_j)\bar{x_j}- \bar{u} \\
        \hat{u}_i &= (\beta_0 - \hat{\beta}_0)+\sum_{j=1}^k(\beta_j-\hat{\beta}_j)x_{ij} + u_i- (\beta_0-\hat{\beta}_0) - \sum_{j=1}^k(\beta_j-\hat{\beta}_j)\bar{x_j}- \bar{u}\\
        \hat{u}_i &= (u_i-\bar{u}) - \sum_{j=1}^k(\hat{\beta}_j-\beta_j)(x_{ij}-\bar{x}_j)
\end{aligned}
\end{equation}
Now, using result (8) : 
\begin{equation}
    \hat{\sigma}^2=\frac{1}{n-k-1}\sum_{i=1}^n[(u_i-\bar{u}) - \sum_{j=1}^k(\hat{\beta}_j-\beta_j)(x_{ij}-\bar{x}_j)]^2
\end{equation}
Let's expand this squared sum to better identify which terms tends to what : 
\begin{equation}
     \hat{\sigma}^2 = \frac{1}{n - k - 1}   \sum_{i=1}^n \left[(u_i - \bar{u})^2    - 2 (u_i-\bar{u})\sum_{j=1}^n(\hat{\beta}_j - \beta_j)(x_{ij} - \bar{x}_j)   + \left( \sum_{j=1}^k (\hat{\beta}_j - \beta_j)(x_{ij} - \bar{x}_j) \right)^2\right]  
\end{equation}
We know that $E[x_iu_i]=0$ and that $E[x_ix_i']$ is non singular. Thus, the conditions are satisfied for the OLS estimator to be consistent. In other words, we can assert that : 
\begin{equation}
    \forall j, \hat{\beta}_j \xrightarrow{P}\beta_j
\end{equation}
Using Slutsky with result (11), we can now state : 
\begin{equation}
    \sum_{j=1}^k (\hat{\beta}_j - \beta_j)(x_{ij} - \bar{x}_j) \xrightarrow{P}\ \sum_{j=1}^k(\beta_j-\beta_j)(x_{ij}-\bar{x}_j) = 0
\end{equation}
Similarly, using the continuous mapping theorem with $g:x \mapsto x^2 $, we get : 
\begin{equation}
     \left( \sum_{j=1}^k (\hat{\beta}_j - \beta_j)(x_{ij} - \bar{x}_j) \right)^2 \xrightarrow{P}0^2=0
\end{equation}
Finally, apllying the Law of Large number to the leftmost term we get : 
\begin{equation}
    \frac{1}{n-k-1}\sum_{i=1}^{n}(u_i-\bar{u})^2 \approx \frac{1}{n}\sum_{i=1}^{n}(u_i-\bar{u})^2\xrightarrow{P} E[(u_i-\bar{u})^2] = Var(u_i)
\end{equation}
Thus, applying the Slutsky theorem with results (10), (12), (13) and (14), we get : 
\begin{equation}
    \hat{\sigma}^2 \xrightarrow{P}Var(u_i) + 0 +0 = \sigma^2
\end{equation}
Thus we can safely state that $\hat{\sigma}^2$ is a consistent estimator for $\sigma^2$.



\end{document}

